# üå± GreenTech Solutions - Analyse √ânerg√©tique Rh√¥ne-Alpes

> Application compl√®te d'analyse et de pr√©diction de performance √©nerg√©tique des logements bas√©e sur les donn√©es DPE (Diagnostic de Performance √ânerg√©tique) et Enedis de la r√©gion Rh√¥ne-Alpes.

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## üìã Table des Mati√®res

- [Vue d'ensemble](#-vue-densemble)
- [Parcours du Projet](#-parcours-du-projet)
- [Fonctionnalit√©s](#-fonctionnalit√©s)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Dockerisation](#-dockerisation)
- [Structure du Projet](#-structure-du-projet)
- [Technologies Utilis√©es](#-technologies-utilis√©es)
- [Mod√®les de Machine Learning](#-mod√®les-de-machine-learning)
- [API REST](#-api-rest)
- [Contributeurs](#-contributeurs)
- [Licence](#-licence)

---

## üéØ Vue d'ensemble

**GreenTech Solutions** est une application full-stack permettant d'analyser, pr√©dire et visualiser les performances √©nerg√©tiques des logements dans le d√©partement du Rh√¥ne (69). Le projet combine **collecte de donn√©es**, **analyse exploratoire**, **machine learning** et **d√©veloppement d'application web** pour cr√©er une solution compl√®te et d√©ployable.

### Objectifs du Projet

- üîç **Analyser** les performances √©nerg√©tiques des logements existants et neufs
- üìä **Visualiser** les donn√©es DPE et Enedis de mani√®re interactive
- ü§ñ **Pr√©dire** l'√©tiquette DPE et le co√ªt √©nerg√©tique d'un logement
- üîÑ **Automatiser** la collecte et la mise √† jour des donn√©es
- üöÄ **D√©ployer** une application web accessible via Docker

---

## üõ§Ô∏è Parcours du Projet

Le projet s'est d√©roul√© en **4 phases principales** :

### Phase 1 : Collecte des Donn√©es üì•
**Notebook 1 : R√©cup√©ration des donn√©es**

#### 1.1 API ADEME - DPE Existants
- **Source** : [API Data ADEME](https://data.ademe.fr/) - Dataset `dpe03existant`
- **M√©thode** : Requ√™tes HTTP avec pagination intelligente
- **Volume** : ~50,000 DPE de logements existants
- **Colonnes** : 160 variables (consommations, caract√©ristiques, √©missions GES)

```python
# Strat√©gie de r√©cup√©ration
for code_postal in codes_postaux_69:
    if total > 10000:
        # D√©coupage par √©tiquette (A, B, C, D, E, F, G)
        # Puis par ann√©e si n√©cessaire
    fetch_data_smart(code_postal)
```

#### 1.2 API ADEME - DPE Neufs
- **Source** : Dataset `dpe02neuf`
- **Volume** : ~5,000 DPE de constructions neuves
- **Colonnes** : 95 variables
- **Particularit√©** : Colonnes diff√©rentes des DPE existants

#### 1.3 API Enedis
- **Source** : Donn√©es de consommation √©lectrique r√©elles
- **Granularit√©** : Par commune et par p√©riode
- **Usage** : Enrichissement et validation des pr√©dictions

#### 1.4 Harmonisation
- **Probl√®me** : Colonnes diff√©rentes entre DPE existants et neufs
- **Solution** : Identification de 80 colonnes communes
- **R√©sultat** : Dataset unifi√© de ~55,000 DPE avec tra√ßabilit√© (colonne `source_dpe`)

**Fichiers g√©n√©r√©s** :
```
data/
‚îú‚îÄ‚îÄ data_existants_69.csv       # DPE existants bruts
‚îú‚îÄ‚îÄ data_neufs_69.csv           # DPE neufs bruts
‚îú‚îÄ‚îÄ data_enedis_69.csv          # Donn√©es Enedis
‚îî‚îÄ‚îÄ donnees_ademe_unifiees.csv  # Donn√©es fusionn√©es
```

---

### Phase 2 : Exploration et Nettoyage üîç
**Notebook 2 : EDA et Preprocessing**

#### 2.1 Analyse Exploratoire (EDA)
- **Analyse descriptive** : Distribution des √©tiquettes, consommations moyennes
- **Visualisations** : 
  - Distribution des √©tiquettes DPE (A-G)
  - Consommation par type de b√¢timent
  - Corr√©lations entre variables
  - Analyse g√©ographique par code postal
- **Insights cl√©s** :
  - 70% des logements ont une √©tiquette D, E ou F
  - Les maisons consomment en moyenne 30% de plus que les appartements
  - Forte corr√©lation entre surface et co√ªt √©nerg√©tique

#### 2.2 Nettoyage des Donn√©es
**Gestion des valeurs manquantes** :
```python
# Strat√©gie par type de variable
- Num√©riques : Imputation par la m√©diane
- Cat√©gorielles : Mode ou cr√©ation cat√©gorie "Inconnu"
- Suppression si > 50% manquant
```

**Traitement des outliers** :
- M√©thode IQR (Interquartile Range)
- Seuils d√©finis par expertise m√©tier (ex: conso > 500 kWh/m¬≤)

**Gestion des doublons** :
- Bas√©e sur `numero_dpe` (identifiant unique)
- Conservation de la version la plus r√©cente

#### 2.3 Feature Engineering
**Nouvelles variables cr√©√©es** :
```python
# Variables calcul√©es
- conso_par_m2 = conso_totale / surface
- ratio_ecs = conso_ecs / conso_totale
- age_batiment = annee_actuelle - annee_construction
- type_energie_recodee (regroupement des √©nergies)
```

**Encodage des variables** :
- Label Encoding : `type_batiment` (maison‚Üí0, appartement‚Üí1, immeuble‚Üí2)
- Label Encoding : `type_energie_recodee` (Electricite‚Üí0, Gaz‚Üí1, etc.)

#### 2.4 S√©lection des Features
**Crit√®res de s√©lection** :
1. Corr√©lation avec la variable cible (> 0.3)
2. Taux de remplissage (> 80%)
3. Importance m√©tier (disponibilit√© lors de la pr√©diction)
4. Variance (exclusion des variables constantes)

**10 Features finales retenues** :
```python
FEATURES = [
    'conso_auxiliaires_ef',           # Consommation auxiliaires
    'cout_eclairage',                 # Co√ªt √©clairage
    'conso_5_usages_par_m2_ef',      # Consommation par m¬≤
    'conso_5_usages_ef',             # Consommation totale
    'surface_habitable_logement',     # Surface
    'cout_ecs',                       # Co√ªt ECS
    'type_batiment',                  # Type (encod√©)
    'conso_ecs_ef',                  # Consommation ECS
    'conso_refroidissement_ef',      # Consommation climatisation
    'type_energie_recodee'           # Type √©nergie (encod√©)
]
```

**Fichier g√©n√©r√©** :
```
data/donnees_ademe_finales_nettoyees_69_final_pret.csv  # Dataset propre
```

---

### Phase 3 : Machine Learning ü§ñ
**Notebook 3 : Mod√©lisation**

#### 3.1 Probl√®mes √† R√©soudre

**Probl√®me 1 : Classification Multi-classes**
- **Variable cible** : `etiquette_dpe` (A, B, C, D, E, F, G)
- **Objectif** : Pr√©dire la classe √©nerg√©tique d'un logement

**Probl√®me 2 : R√©gression**
- **Variable cible** : `cout_total_5_usages` (en ‚Ç¨/an)
- **Objectif** : Estimer le co√ªt √©nerg√©tique annuel

#### 3.2 Pr√©paration des Donn√©es

**Split Train/Test** :
```python
# 80% entra√Ænement / 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42,
    stratify=y  # Pour la classification uniquement
)

# Tailles r√©sultats :
- Train : ~44,000 √©chantillons
- Test  : ~11,000 √©chantillons
```

**Normalisation** :
- Non appliqu√©e pour Random Forest (mod√®le bas√© sur les arbres)
- Les RF sont invariants aux √©chelles des features

#### 3.3 Mod√®le de Classification

**Algorithme choisi** : Random Forest Classifier

**Raisons du choix** :
- ‚úÖ G√®re bien les donn√©es non-lin√©aires
- ‚úÖ Robuste aux outliers
- ‚úÖ Importance des features interpr√©table
- ‚úÖ Pas de normalisation n√©cessaire
- ‚úÖ Performances √©lev√©es en multi-classes

**Hyperparam√®tres optimis√©s** :
```python
RandomForestClassifier(
    n_estimators=100,        # 100 arbres
    max_depth=20,            # Profondeur max
    min_samples_split=5,     # Min √©chantillons pour split
    min_samples_leaf=2,      # Min √©chantillons par feuille
    random_state=42,
    n_jobs=-1               # Parall√©lisation
)
```

**R√©sultats** :
```
Accuracy      : 98.06%
Precision     : 0.98 (moyenne pond√©r√©e)
Recall        : 0.98 (moyenne pond√©r√©e)
F1-Score      : 0.97 (moyenne pond√©r√©e)

Matrice de confusion :
       A    B    C    D    E    F    G
A   [450   2    0    0    0    0    0]
B   [  1 890   10   0    0    0    0]
C   [  0   8 1850  12   0    0    0]
D   [  0   0   15 3200  20   0    0]
E   [  0   0    0   18 2850  15   0]
F   [  0   0    0    0   12 1780   8]
G   [  0   0    0    0    0    5  995]
```

**Importance des features** :
```
1. conso_5_usages_par_m2_ef    (0.35)  ‚Üê Plus important
2. conso_5_usages_ef           (0.22)
3. surface_habitable_logement  (0.15)
4. cout_ecs                    (0.10)
5. type_energie_recodee        (0.08)
...
```

#### 3.4 Mod√®le de R√©gression

**Algorithme choisi** : Random Forest Regressor

**Hyperparam√®tres optimis√©s** :
```python
RandomForestRegressor(
    n_estimators=100,
    max_depth=20,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
```

**R√©sultats** :
```
R¬≤ Score      : 0.979  (97.9% de variance expliqu√©e)
MAE           : 89.5 ‚Ç¨ (erreur moyenne absolue)
RMSE          : 142.3 ‚Ç¨ (erreur quadratique moyenne)
MAPE          : 8.2%   (erreur moyenne en pourcentage)

Intervalles de confiance √† 95% : ¬±176 ‚Ç¨
```

**Analyse des r√©sidus** :
- Distribution normale centr√©e sur 0 ‚úÖ
- Homosc√©dasticit√© v√©rifi√©e ‚úÖ
- Pas de pattern dans les r√©sidus ‚úÖ

#### 3.5 Validation Crois√©e

```python
# K-Fold Cross-Validation (k=5)
Classification :
- Scores CV : [0.978, 0.981, 0.979, 0.982, 0.980]
- Moyenne   : 0.980 ¬± 0.001

R√©gression :
- Scores CV : [0.977, 0.979, 0.978, 0.980, 0.979]
- Moyenne   : 0.979 ¬± 0.001
```

**Conclusion** : Mod√®les robustes et g√©n√©ralisables ‚úÖ

#### 3.6 Sauvegarde des Mod√®les

```python
import joblib

# Sauvegarde
joblib.dump(classifier, 'models/classification_model.pkl')
joblib.dump(regressor, 'models/regression_model.pkl')

# M√©tadonn√©es
metrics = {
    'classification': {
        'accuracy': 0.9806,
        'f1_score': 0.97,
        'trained_at': '2024-12-20',
        'train_samples': 44000
    },
    'regression': {
        'r2_score': 0.979,
        'mae': 89.5,
        'trained_at': '2024-12-20',
        'train_samples': 44000
    }
}
```

**Fichiers g√©n√©r√©s** :
```
models/
‚îú‚îÄ‚îÄ classification_model.pkl  (~15 MB)
‚îú‚îÄ‚îÄ regression_model.pkl      (~12 MB)
‚îî‚îÄ‚îÄ metrics.json              (~2 KB)
```

---

### Phase 4 : D√©veloppement de l'Application üíª

#### 4.1 Architecture Full-Stack

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  FRONTEND                            ‚îÇ
‚îÇ  Streamlit (Port 8501)                              ‚îÇ
‚îÇ  ‚îú‚îÄ üè† Accueil                                      ‚îÇ
‚îÇ  ‚îú‚îÄ üìä Tableau de bord                              ‚îÇ
‚îÇ  ‚îú‚îÄ üìà Analyses                                     ‚îÇ
‚îÇ  ‚îú‚îÄ ‚ö° Donn√©es Enedis                               ‚îÇ
‚îÇ  ‚îú‚îÄ üîÆ Pr√©diction                                   ‚îÇ
‚îÇ  ‚îú‚îÄ ‚öñÔ∏è Comparaison                                  ‚îÇ
‚îÇ  ‚îú‚îÄ üîÑ Rafra√Æchir donn√©es ‚Üê Nouveau                ‚îÇ
‚îÇ  ‚îú‚îÄ üéØ R√©entra√Æner mod√®les ‚Üê Nouveau               ‚îÇ
‚îÇ  ‚îî‚îÄ üì° API Interface ‚Üê Nouveau                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚Üì HTTP REST
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  BACKEND                             ‚îÇ
‚îÇ  FastAPI (Port 8000)                                ‚îÇ
‚îÇ  ‚îú‚îÄ GET  /health                                    ‚îÇ
‚îÇ  ‚îú‚îÄ POST /predict                                   ‚îÇ
‚îÇ  ‚îú‚îÄ POST /predict/batch                             ‚îÇ
‚îÇ  ‚îú‚îÄ GET  /models/metrics                            ‚îÇ
‚îÇ  ‚îú‚îÄ POST /models/retrain                            ‚îÇ
‚îÇ  ‚îî‚îÄ POST /data/refresh                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              DONN√âES & MOD√àLES                       ‚îÇ
‚îÇ  ‚îú‚îÄ data/                                           ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ donnees_ademe_finales_*.csv                 ‚îÇ
‚îÇ  ‚îî‚îÄ models/                                         ‚îÇ
‚îÇ     ‚îú‚îÄ classification_model.pkl                     ‚îÇ
‚îÇ     ‚îî‚îÄ regression_model.pkl                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 4.2 Interface Utilisateur (Streamlit)

**Pages d√©velopp√©es** :

1. **üè† Accueil** : Pr√©sentation du projet et KPIs
2. **üìä Tableau de bord** : Vue d'ensemble des donn√©es avec filtres interactifs
3. **üìà Analyse** : Analyses statistiques approfondies
4. **‚ö° Enedis** : Int√©gration des donn√©es de consommation Enedis
5. **üîÆ Pr√©diction** : 
   - Pr√©diction individuelle avec formulaire
   - Pr√©diction par lot (upload CSV)
   - Visualisations des r√©sultats
6. **‚öñÔ∏è Comparaison** : Comparaison de plusieurs logements
7. **üîÑ Rafra√Æchir donn√©es** : 
   - Mode incr√©mental (nouveaux DPE uniquement)
   - Mode complet (rechargement total)
   - Gestion des 2 sources (existants + neufs)
8. **üéØ R√©entra√Æner mod√®les** :
   - Configuration des hyperparam√®tres
   - Suivi de l'entra√Ænement en temps r√©el
   - Visualisation des performances
9. **üì° API Interface** : Testeur interactif pour l'API

**Technologies UI** :
- Streamlit pour l'interface
- Plotly pour les graphiques interactifs
- CSS personnalis√© pour le design

#### 4.3 API REST (FastAPI)

**Endpoints d√©velopp√©s** :

```python
# Sant√© de l'API
GET /health
‚Üí {"status": "healthy", "models_loaded": true}

# Pr√©diction individuelle
POST /predict
Body: {
    "conso_auxiliaires_ef": 500,
    "cout_eclairage": 80,
    ...
}
‚Üí {
    "etiquette_dpe": "D",
    "cout_total_5_usages": 1234.56,
    "probabilities": {...},
    "timestamp": "2024-12-20T10:30:00"
}

# Pr√©dictions multiples
POST /predict/batch
Body: {"data": [{...}, {...}]}
‚Üí {"predictions": [...], "total": 10}

# M√©triques des mod√®les
GET /models/metrics
‚Üí {
    "classification": {"accuracy": 0.98, ...},
    "regression": {"r2_score": 0.979, ...}
}

# R√©entra√Ænement (t√¢che asynchrone)
POST /models/retrain
‚Üí {"status": "started", "message": "..."}

# Rafra√Æchissement donn√©es (t√¢che asynchrone)
POST /data/refresh?full_reload=false
‚Üí {"status": "success", "new_records": 1200}
```

**Features** :
- Validation automatique avec Pydantic
- Documentation Swagger auto-g√©n√©r√©e
- Gestion des erreurs structur√©e
- CORS configur√©
- T√¢ches de fond (BackgroundTasks)

#### 4.4 Modules Utilitaires

**`utils/data_refresher_complete.py`** :
- R√©cup√©ration automatique des donn√©es ADEME
- Gestion des 2 sources (existants + neufs)
- D√©coupage intelligent si > 10,000 r√©sultats
- Harmonisation et fusion des datasets
- Mode incr√©mental vs mode complet

**`utils/model_trainer.py`** :
- Pipeline d'entra√Ænement complet
- Pr√©paration et encodage des donn√©es
- Entra√Ænement des 2 mod√®les
- Calcul des m√©triques
- Sauvegarde automatique

**`utils/api_client.py`** :
- Client HTTP pour l'API FastAPI
- Utilis√© par Streamlit pour appeler l'API
- Gestion des erreurs et timeouts

---

### Phase 5 : Dockerisation üê≥

#### 5.1 Strat√©gie de Conteneurisation

**Architecture multi-services** :
```
Docker Compose
‚îú‚îÄ Service Streamlit (Port 8501)
‚îÇ  ‚îî‚îÄ Interface utilisateur
‚îî‚îÄ Service API (Port 8000)
   ‚îî‚îÄ Backend FastAPI
```

#### 5.2 Dockerfile

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Installation des d√©pendances
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copie du code
COPY . .

# Cr√©ation des dossiers
RUN mkdir -p data models logs

# Exposition des ports
EXPOSE 8501 8000

# Script d'entr√©e
ENTRYPOINT ["/docker-entrypoint.sh"]
```

#### 5.3 Docker Compose

```yaml
version: '3.8'

services:
  streamlit:
    build: .
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - SERVICE_MODE=streamlit
    command: streamlit run app.py --server.address=0.0.0.0

  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - SERVICE_MODE=api
    command: uvicorn api.main:app --host 0.0.0.0 --port 8000
```

#### 5.4 Volumes Persistants

```
Machine H√¥te          Conteneur Docker
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
./data/          ‚Üí    /app/data/
./models/        ‚Üí    /app/models/
./logs/          ‚Üí    /app/logs/

Avantages :
‚îú‚îÄ Donn√©es persistent apr√®s red√©marrage
‚îú‚îÄ Mod√®les persistent apr√®s red√©marrage
‚îî‚îÄ Pas besoin de rebuild si donn√©es changent
```

---

## ‚ú® Fonctionnalit√©s

### üîç Analyse de Donn√©es
- Visualisations interactives (Plotly)
- Filtres dynamiques par code postal, type de b√¢timent, √©tiquette
- Statistiques descriptives
- Analyse g√©ographique

### ü§ñ Machine Learning
- **Classification** : Pr√©diction de l'√©tiquette DPE (A-G) avec 98% de pr√©cision
- **R√©gression** : Estimation du co√ªt √©nerg√©tique avec R¬≤=0.979
- Pr√©dictions individuelles et par lot
- Intervalles de confiance

### üîÑ Gestion des Donn√©es
- Rafra√Æchissement automatique depuis l'API ADEME
- Mode incr√©mental (nouveaux DPE uniquement)
- Mode complet (rechargement total)
- Gestion de 2 sources (existants + neufs)
- Fusion et harmonisation automatiques

### üéØ R√©entra√Ænement
- Interface pour r√©entra√Æner les mod√®les
- Configuration des hyperparam√®tres
- Visualisation des performances en temps r√©el
- Sauvegarde automatique

### üì° API REST
- 8 endpoints document√©s (Swagger)
- Pr√©dictions via HTTP POST
- M√©triques des mod√®les
- Gestion asynchrone

---

## üèóÔ∏è Architecture

### Stack Technique

```
Frontend  : Streamlit 1.28+
Backend   : FastAPI 0.104+
ML        : Scikit-learn 1.3+
Viz       : Plotly 5.17+
Data      : Pandas 2.1+, NumPy 1.25+
Deploy    : Docker, Docker Compose
```

### Flux de Donn√©es

```
API ADEME ‚Üí CSV ‚Üí Nettoyage ‚Üí ML ‚Üí Mod√®les .pkl ‚Üí API ‚Üí Interface
   ‚Üì                                     ‚Üë
Notebooks                          R√©entra√Ænement
```

---

## üöÄ Installation

### Pr√©requis

- Python 3.10+
- pip
- Docker & Docker Compose (optionnel)
- 4 GB RAM minimum
- 2 GB d'espace disque

### Installation Locale

```bash
# 1. Cloner le repository
git clone https://github.com/votre-username/greentech-solutions.git
cd greentech-solutions

# 2. Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# 3. Installer les d√©pendances
pip install -r requirements.txt

# 4. Cr√©er les dossiers n√©cessaires
mkdir -p data models logs
```

### Installation Docker

```bash
# 1. Cloner le repository
git clone https://github.com/votre-username/greentech-solutions.git
cd greentech-solutions

# 2. Construire les images
docker-compose build

# 3. Lancer les services
docker-compose up -d
```

---

## üíª Utilisation

### Lancement Local

```bash
# Terminal 1 : Lancer Streamlit
streamlit run app.py

# Terminal 2 : Lancer l'API
uvicorn api.main:app --reload
```

Puis ouvrir :
- **Streamlit** : http://localhost:8501
- **API Docs** : http://localhost:8000/docs

### Lancement Docker

```bash
# D√©marrer tous les services
docker-compose up -d

# Voir les logs
docker-compose logs -f

# Arr√™ter les services
docker-compose down
```

### Workflow Typique

```bash
# 1. Premi√®re utilisation : Entra√Æner les mod√®les
‚Üí Ouvrir http://localhost:8501
‚Üí Aller sur "üéØ R√©entra√Æner mod√®les"
‚Üí Cliquer "Lancer l'entra√Ænement"
‚Üí Attendre 1-2 minutes

# 2. Faire des pr√©dictions
‚Üí Aller sur "üîÆ Pr√©diction"
‚Üí Remplir le formulaire
‚Üí Cliquer "Lancer la pr√©diction"

# 3. Rafra√Æchir les donn√©es (optionnel)
‚Üí Aller sur "üîÑ Rafra√Æchir donn√©es"
‚Üí Choisir mode incr√©mental ou complet
‚Üí Lancer le rafra√Æchissement

# 4. Utiliser l'API
‚Üí Ouvrir http://localhost:8000/docs
‚Üí Tester les endpoints interactivement
```

---

## üê≥ Dockerisation

### Commandes Utiles

```bash
# Construire
docker-compose build

# D√©marrer
docker-compose up -d

# Arr√™ter
docker-compose down

# Logs
docker-compose logs -f streamlit
docker-compose logs -f api

# Red√©marrer un service
docker-compose restart api

# Entrer dans un conteneur
docker-compose exec streamlit bash

# Reconstruire et red√©marrer
docker-compose up -d --build
```

### Push vers Docker Hub

```bash
# Tag
docker tag greentech-app:latest username/greentech-app:latest

# Push
docker push username/greentech-app:latest

# Pull (sur un autre serveur)
docker pull username/greentech-app:latest
docker-compose up -d
```

---

## üìÅ Structure du Projet

```
greentech-solutions/
‚îÇ
‚îú‚îÄ‚îÄ üìì notebooks/                    # Notebooks Jupyter
‚îÇ   ‚îú‚îÄ‚îÄ 01_collecte_donnees.ipynb   # R√©cup√©ration API ADEME + Enedis
‚îÇ   ‚îú‚îÄ‚îÄ 02_exploration_nettoyage.ipynb  # EDA et preprocessing
‚îÇ   ‚îî‚îÄ‚îÄ 03_modelisation.ipynb       # Classification et r√©gression
‚îÇ
‚îú‚îÄ‚îÄ üé® pages/                        # Pages Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ welcome.py                  # Page d'accueil
‚îÇ   ‚îú‚îÄ‚îÄ home.py                     # Tableau de bord
‚îÇ   ‚îú‚îÄ‚îÄ analysis.py                 # Analyses
‚îÇ   ‚îú‚îÄ‚îÄ enedis.py                   # Donn√©es Enedis
‚îÇ   ‚îú‚îÄ‚îÄ prediction.py               # Pr√©dictions
‚îÇ   ‚îú‚îÄ‚îÄ compare.py                  # Comparaisons
‚îÇ   ‚îú‚îÄ‚îÄ refresh_data.py             # Rafra√Æchissement donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ retrain_models.py           # R√©entra√Ænement mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ api_interface.py            # Interface API
‚îÇ   ‚îî‚îÄ‚îÄ about.py                    # √Ä propos
‚îÇ
‚îú‚îÄ‚îÄ üîß utils/                        # Modules utilitaires
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py              # Chargement donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ model_utils.py              # Utilitaires mod√®les
‚îÇ   ‚îú‚îÄ‚îÄ data_refresher_complete.py  # Rafra√Æchissement API
‚îÇ   ‚îú‚îÄ‚îÄ model_trainer.py            # Entra√Ænement mod√®les
‚îÇ   ‚îî‚îÄ‚îÄ api_client.py               # Client API
‚îÇ
‚îú‚îÄ‚îÄ üîå api/                          # API FastAPI
‚îÇ   ‚îî‚îÄ‚îÄ main.py                     # Endpoints REST
‚îÇ
‚îú‚îÄ‚îÄ ü§ñ models/                       # Mod√®les ML
‚îÇ   ‚îú‚îÄ‚îÄ classification_model.pkl    # Mod√®le classification
‚îÇ   ‚îú‚îÄ‚îÄ regression_model.pkl        # Mod√®le r√©gression
‚îÇ   ‚îî‚îÄ‚îÄ metrics.json                # M√©triques
‚îÇ
‚îú‚îÄ‚îÄ üìä data/                         # Donn√©es
‚îÇ   ‚îú‚îÄ‚îÄ donnees_ademe_finales_*.csv # Dataset principal
‚îÇ   ‚îú‚îÄ‚îÄ adresses-69.csv             # Codes postaux
‚îÇ   ‚îî‚îÄ‚îÄ metadata.json               # M√©tadonn√©es
‚îÇ
‚îú‚îÄ‚îÄ üê≥ Docker/                       # Configuration Docker
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                  # Image Docker
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml          # Orchestration
‚îÇ   ‚îî‚îÄ‚îÄ docker-entrypoint.sh        # Script d√©marrage
‚îÇ
‚îú‚îÄ‚îÄ üì± app.py                        # Application principale
‚îú‚îÄ‚îÄ üìã requirements.txt              # D√©pendances Python
‚îú‚îÄ‚îÄ üß™ test_api.py                  # Tests API
‚îî‚îÄ‚îÄ üìñ README.md                    # Ce fichier
```

---

## üõ†Ô∏è Technologies Utilis√©es

### Backend & API
![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green?logo=fastapi)
![Pandas](https://img.shields.io/badge/Pandas-2.1-orange?logo=pandas)
![NumP
