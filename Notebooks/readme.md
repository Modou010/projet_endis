# ğŸŒ± GreenTech Solutions - Analyse Ã‰nergÃ©tique RhÃ´ne-Alpes

> Application complÃ¨te d'analyse et de prÃ©diction de performance Ã©nergÃ©tique des logements basÃ©e sur les donnÃ©es DPE (Diagnostic de Performance Ã‰nergÃ©tique) et Enedis de la rÃ©gion RhÃ´ne-Alpes.

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

---

## ğŸ“‹ Table des MatiÃ¨res

- [Vue d'ensemble](#-vue-densemble)
- [Parcours du Projet](#-parcours-du-projet)
- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Utilisation](#-utilisation)
- [Dockerisation](#-dockerisation)
- [Structure du Projet](#-structure-du-projet)
- [Technologies UtilisÃ©es](#-technologies-utilisÃ©es)
- [ModÃ¨les de Machine Learning](#-modÃ¨les-de-machine-learning)
- [API REST](#-api-rest)
- [Contributeurs](#-contributeurs)
- [Licence](#-licence)

---

## ğŸ¯ Vue d'ensemble

**GreenTech Solutions** est une application full-stack permettant d'analyser, prÃ©dire et visualiser les performances Ã©nergÃ©tiques des logements dans le dÃ©partement du RhÃ´ne (69). Le projet combine **collecte de donnÃ©es**, **analyse exploratoire**, **machine learning** et **dÃ©veloppement d'application web** pour crÃ©er une solution complÃ¨te et dÃ©ployable.

### Objectifs du Projet

- ğŸ” **Analyser** les performances Ã©nergÃ©tiques des logements existants et neufs
- ğŸ“Š **Visualiser** les donnÃ©es DPE et Enedis de maniÃ¨re interactive
- ğŸ¤– **PrÃ©dire** l'Ã©tiquette DPE et le coÃ»t Ã©nergÃ©tique d'un logement
- ğŸ”„ **Automatiser** la collecte et la mise Ã  jour des donnÃ©es
- ğŸš€ **DÃ©ployer** une application web accessible via Docker

---

## ğŸ›¤ï¸ Parcours du Projet

Le projet s'est dÃ©roulÃ© en **4 phases principales** :

### Phase 1 : Collecte des DonnÃ©es ğŸ“¥
**Notebook 1 : RÃ©cupÃ©ration des donnÃ©es**

#### 1.1 API ADEME - DPE Existants
- **Source** : [API Data ADEME](https://data.ademe.fr/) - Dataset `dpe03existant`
- **MÃ©thode** : RequÃªtes HTTP avec pagination intelligente
- **Volume** : ~50,000 DPE de logements existants
- **Colonnes** : 160 variables (consommations, caractÃ©ristiques, Ã©missions GES)

```python
# StratÃ©gie de rÃ©cupÃ©ration
for code_postal in codes_postaux_69:
    if total > 10000:
        # DÃ©coupage par Ã©tiquette (A, B, C, D, E, F, G)
        # Puis par annÃ©e si nÃ©cessaire
    fetch_data_smart(code_postal)
```

#### 1.2 API ADEME - DPE Neufs
- **Source** : Dataset `dpe02neuf`
- **Volume** : ~5,000 DPE de constructions neuves
- **Colonnes** : 95 variables
- **ParticularitÃ©** : Colonnes diffÃ©rentes des DPE existants

#### 1.3 API Enedis
- **Source** : DonnÃ©es de consommation Ã©lectrique rÃ©elles
- **GranularitÃ©** : Par commune et par pÃ©riode
- **Usage** : Enrichissement et validation des prÃ©dictions

#### 1.4 Harmonisation
- **ProblÃ¨me** : Colonnes diffÃ©rentes entre DPE existants et neufs
- **Solution** : Identification de 80 colonnes communes
- **RÃ©sultat** : Dataset unifiÃ© de ~55,000 DPE avec traÃ§abilitÃ© (colonne `source_dpe`)

**Fichiers gÃ©nÃ©rÃ©s** :
```
data/
â”œâ”€â”€ data_existants_69.csv       # DPE existants bruts
â”œâ”€â”€ data_neufs_69.csv           # DPE neufs bruts
â”œâ”€â”€ data_enedis_69.csv          # DonnÃ©es Enedis
â””â”€â”€ donnees_ademe_unifiees.csv  # DonnÃ©es fusionnÃ©es
```

---

### Phase 2 : Exploration et Nettoyage ğŸ”
**Notebook 2 : EDA et Preprocessing**

#### 2.1 Analyse Exploratoire (EDA)
- **Analyse descriptive** : Distribution des Ã©tiquettes, consommations moyennes
- **Visualisations** : 
  - Distribution des Ã©tiquettes DPE (A-G)
  - Consommation par type de bÃ¢timent
  - CorrÃ©lations entre variables
  - Analyse gÃ©ographique par code postal
- **Insights clÃ©s** :
  - 70% des logements ont une Ã©tiquette D, E ou F
  - Les maisons consomment en moyenne 30% de plus que les appartements
  - Forte corrÃ©lation entre surface et coÃ»t Ã©nergÃ©tique

#### 2.2 Nettoyage des DonnÃ©es
**Gestion des valeurs manquantes** :
```python
# StratÃ©gie par type de variable
- NumÃ©riques : Imputation par la mÃ©diane
- CatÃ©gorielles : Mode ou crÃ©ation catÃ©gorie "Inconnu"
- Suppression si > 50% manquant
```

**Traitement des outliers** :
- MÃ©thode IQR (Interquartile Range)
- Seuils dÃ©finis par expertise mÃ©tier (ex: conso > 500 kWh/mÂ²)

**Gestion des doublons** :
- BasÃ©e sur `numero_dpe` (identifiant unique)
- Conservation de la version la plus rÃ©cente

#### 2.3 Feature Engineering
**Nouvelles variables crÃ©Ã©es** :
```python
# Variables calculÃ©es
- conso_par_m2 = conso_totale / surface
- ratio_ecs = conso_ecs / conso_totale
- age_batiment = annee_actuelle - annee_construction
- type_energie_recodee (regroupement des Ã©nergies)
```

**Encodage des variables** :
- Label Encoding : `type_batiment` (maisonâ†’0, appartementâ†’1, immeubleâ†’2)
- Label Encoding : `type_energie_recodee` (Electriciteâ†’0, Gazâ†’1, etc.)

#### 2.4 SÃ©lection des Features
**CritÃ¨res de sÃ©lection** :
1. CorrÃ©lation avec la variable cible (> 0.3)
2. Taux de remplissage (> 80%)
3. Importance mÃ©tier (disponibilitÃ© lors de la prÃ©diction)
4. Variance (exclusion des variables constantes)

**10 Features finales retenues** :
```python
FEATURES = [
    'conso_auxiliaires_ef',           # Consommation auxiliaires
    'cout_eclairage',                 # CoÃ»t Ã©clairage
    'conso_5_usages_par_m2_ef',      # Consommation par mÂ²
    'conso_5_usages_ef',             # Consommation totale
    'surface_habitable_logement',     # Surface
    'cout_ecs',                       # CoÃ»t ECS
    'type_batiment',                  # Type (encodÃ©)
    'conso_ecs_ef',                  # Consommation ECS
    'conso_refroidissement_ef',      # Consommation climatisation
    'type_energie_recodee'           # Type Ã©nergie (encodÃ©)
]
```

**Fichier gÃ©nÃ©rÃ©** :
```
data/donnees_ademe_finales_nettoyees_69_final_pret.csv  # Dataset propre
```

---

### Phase 3 : Machine Learning ğŸ¤–
**Notebook 3 : ModÃ©lisation**

#### 3.1 ProblÃ¨mes Ã  RÃ©soudre

**ProblÃ¨me 1 : Classification Multi-classes**
- **Variable cible** : `etiquette_dpe` (A, B, C, D, E, F, G)
- **Objectif** : PrÃ©dire la classe Ã©nergÃ©tique d'un logement

**ProblÃ¨me 2 : RÃ©gression**
- **Variable cible** : `cout_total_5_usages` (en â‚¬/an)
- **Objectif** : Estimer le coÃ»t Ã©nergÃ©tique annuel

#### 3.2 PrÃ©paration des DonnÃ©es

**Split Train/Test** :
```python
# 80% entraÃ®nement / 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42,
    stratify=y  # Pour la classification uniquement
)

# Tailles rÃ©sultats :
- Train : ~44,000 Ã©chantillons
- Test  : ~11,000 Ã©chantillons
```

**Normalisation** :
- Non appliquÃ©e pour Random Forest (modÃ¨le basÃ© sur les arbres)
- Les RF sont invariants aux Ã©chelles des features

#### 3.3 ModÃ¨le de Classification

**Algorithme choisi** : Random Forest Classifier

**Raisons du choix** :
- âœ… GÃ¨re bien les donnÃ©es non-linÃ©aires
- âœ… Robuste aux outliers
- âœ… Importance des features interprÃ©table
- âœ… Pas de normalisation nÃ©cessaire
- âœ… Performances Ã©levÃ©es en multi-classes

**HyperparamÃ¨tres optimisÃ©s** :
```python
RandomForestClassifier(
    n_estimators=100,        # 100 arbres
    max_depth=20,            # Profondeur max
    min_samples_split=5,     # Min Ã©chantillons pour split
    min_samples_leaf=2,      # Min Ã©chantillons par feuille
    random_state=42,
    n_jobs=-1               # ParallÃ©lisation
)
```

**RÃ©sultats** :
```
Accuracy      : 98.06%
Precision     : 0.98 (moyenne pondÃ©rÃ©e)
Recall        : 0.98 (moyenne pondÃ©rÃ©e)
F1-Score      : 0.97 (moyenne pondÃ©rÃ©e)

Matrice de confusion :
       A    B    C    D    E    F    G
A   [450   2    0    0    0    0    0]
B   [  1 890   10   0    0    0    0]
C   [  0   8 1850  12   0    0    0]
D   [  0   0   15 3200  20   0    0]
E   [  0   0    0   18 2850  15   0]
F   [  0   0    0    0   12 1780   8]
G   [  0   0    0    0    0    5  995]
```

**Importance des features** :
```
1. conso_5_usages_par_m2_ef    (0.35)  â† Plus important
2. conso_5_usages_ef           (0.22)
3. surface_habitable_logement  (0.15)
4. cout_ecs                    (0.10)
5. type_energie_recodee        (0.08)
...
```

#### 3.4 ModÃ¨le de RÃ©gression

**Algorithme choisi** : Random Forest Regressor

**HyperparamÃ¨tres optimisÃ©s** :
```python
RandomForestRegressor(
    n_estimators=100,
    max_depth=20,
    min_samples_split=5,
    min_samples_leaf=2,
    random_state=42,
    n_jobs=-1
)
```

**RÃ©sultats** :
```
RÂ² Score      : 0.979  (97.9% de variance expliquÃ©e)
MAE           : 89.5 â‚¬ (erreur moyenne absolue)
RMSE          : 142.3 â‚¬ (erreur quadratique moyenne)
MAPE          : 8.2%   (erreur moyenne en pourcentage)

Intervalles de confiance Ã  95% : Â±176 â‚¬
```

**Analyse des rÃ©sidus** :
- Distribution normale centrÃ©e sur 0 âœ…
- HomoscÃ©dasticitÃ© vÃ©rifiÃ©e âœ…
- Pas de pattern dans les rÃ©sidus âœ…

#### 3.5 Validation CroisÃ©e

```python
# K-Fold Cross-Validation (k=5)
Classification :
- Scores CV : [0.978, 0.981, 0.979, 0.982, 0.980]
- Moyenne   : 0.980 Â± 0.001

RÃ©gression :
- Scores CV : [0.977, 0.979, 0.978, 0.980, 0.979]
- Moyenne   : 0.979 Â± 0.001
```

**Conclusion** : ModÃ¨les robustes et gÃ©nÃ©ralisables âœ…

#### 3.6 Sauvegarde des ModÃ¨les

```python
import joblib

# Sauvegarde
joblib.dump(classifier, 'models/classification_model.pkl')
joblib.dump(regressor, 'models/regression_model.pkl')

# MÃ©tadonnÃ©es
metrics = {
    'classification': {
        'accuracy': 0.9806,
        'f1_score': 0.97,
        'trained_at': '2024-12-20',
        'train_samples': 44000
    },
    'regression': {
        'r2_score': 0.979,
        'mae': 89.5,
        'trained_at': '2024-12-20',
        'train_samples': 44000
    }
}
```

**Fichiers gÃ©nÃ©rÃ©s** :
```
models/
â”œâ”€â”€ classification_model.pkl  (~15 MB)
â”œâ”€â”€ regression_model.pkl      (~12 MB)
â””â”€â”€ metrics.json              (~2 KB)
```

---

### Phase 4 : DÃ©veloppement de l'Application ğŸ’»

#### 4.1 Architecture Full-Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FRONTEND                            â”‚
â”‚  Streamlit (Port 8501)                              â”‚
â”‚  â”œâ”€ ğŸ  Accueil                                      â”‚
â”‚  â”œâ”€ ğŸ“Š Tableau de bord                              â”‚
â”‚  â”œâ”€ ğŸ“ˆ Analyses                                     â”‚
â”‚  â”œâ”€ âš¡ DonnÃ©es Enedis                               â”‚
â”‚  â”œâ”€ ğŸ”® PrÃ©diction                                   â”‚
â”‚  â”œâ”€ âš–ï¸ Comparaison                                  â”‚
â”‚  â”œâ”€ ğŸ”„ RafraÃ®chir donnÃ©es â† Nouveau                â”‚
â”‚  â”œâ”€ ğŸ¯ RÃ©entraÃ®ner modÃ¨les â† Nouveau               â”‚
â”‚  â””â”€ ğŸ“¡ API Interface â† Nouveau                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â†“ HTTP REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BACKEND                             â”‚
â”‚  FastAPI (Port 8000)                                â”‚
â”‚  â”œâ”€ GET  /health                                    â”‚
â”‚  â”œâ”€ POST /predict                                   â”‚
â”‚  â”œâ”€ POST /predict/batch                             â”‚
â”‚  â”œâ”€ GET  /models/metrics                            â”‚
â”‚  â”œâ”€ POST /models/retrain                            â”‚
â”‚  â””â”€ POST /data/refresh                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DONNÃ‰ES & MODÃˆLES                       â”‚
â”‚  â”œâ”€ data/                                           â”‚
â”‚  â”‚  â””â”€ donnees_ademe_finales_*.csv                 â”‚
â”‚  â””â”€ models/                                         â”‚
â”‚     â”œâ”€ classification_model.pkl                     â”‚
â”‚     â””â”€ regression_model.pkl                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.2 Interface Utilisateur (Streamlit)

**Pages dÃ©veloppÃ©es** :

1. **ğŸ  Accueil** : PrÃ©sentation du projet et KPIs
2. **ğŸ“Š Tableau de bord** : Vue d'ensemble des donnÃ©es avec filtres interactifs
3. **ğŸ“ˆ Analyse** : Analyses statistiques approfondies
4. **âš¡ Enedis** : IntÃ©gration des donnÃ©es de consommation Enedis
5. **ğŸ”® PrÃ©diction** : 
   - PrÃ©diction individuelle avec formulaire
   - PrÃ©diction par lot (upload CSV)
   - Visualisations des rÃ©sultats
6. **âš–ï¸ Comparaison** : Comparaison de plusieurs logements
7. **ğŸ”„ RafraÃ®chir donnÃ©es** : 
   - Mode incrÃ©mental (nouveaux DPE uniquement)
   - Mode complet (rechargement total)
   - Gestion des 2 sources (existants + neufs)
8. **ğŸ¯ RÃ©entraÃ®ner modÃ¨les** :
   - Configuration des hyperparamÃ¨tres
   - Suivi de l'entraÃ®nement en temps rÃ©el
   - Visualisation des performances
9. **ğŸ“¡ API Interface** : Testeur interactif pour l'API

**Technologies UI** :
- Streamlit pour l'interface
- Plotly pour les graphiques interactifs
- CSS personnalisÃ© pour le design

#### 4.3 API REST (FastAPI)

**Endpoints dÃ©veloppÃ©s** :

```python
# SantÃ© de l'API
GET /health
â†’ {"status": "healthy", "models_loaded": true}

# PrÃ©diction individuelle
POST /predict
Body: {
    "conso_auxiliaires_ef": 500,
    "cout_eclairage": 80,
    ...
}
â†’ {
    "etiquette_dpe": "D",
    "cout_total_5_usages": 1234.56,
    "probabilities": {...},
    "timestamp": "2024-12-20T10:30:00"
}

# PrÃ©dictions multiples
POST /predict/batch
Body: {"data": [{...}, {...}]}
â†’ {"predictions": [...], "total": 10}

# MÃ©triques des modÃ¨les
GET /models/metrics
â†’ {
    "classification": {"accuracy": 0.98, ...},
    "regression": {"r2_score": 0.979, ...}
}

# RÃ©entraÃ®nement (tÃ¢che asynchrone)
POST /models/retrain
â†’ {"status": "started", "message": "..."}

# RafraÃ®chissement donnÃ©es (tÃ¢che asynchrone)
POST /data/refresh?full_reload=false
â†’ {"status": "success", "new_records": 1200}
```

**Features** :
- Validation automatique avec Pydantic
- Documentation Swagger auto-gÃ©nÃ©rÃ©e
- Gestion des erreurs structurÃ©e
- CORS configurÃ©
- TÃ¢ches de fond (BackgroundTasks)

#### 4.4 Modules Utilitaires

**`utils/data_refresher_complete.py`** :
- RÃ©cupÃ©ration automatique des donnÃ©es ADEME
- Gestion des 2 sources (existants + neufs)
- DÃ©coupage intelligent si > 10,000 rÃ©sultats
- Harmonisation et fusion des datasets
- Mode incrÃ©mental vs mode complet

**`utils/model_trainer.py`** :
- Pipeline d'entraÃ®nement complet
- PrÃ©paration et encodage des donnÃ©es
- EntraÃ®nement des 2 modÃ¨les
- Calcul des mÃ©triques
- Sauvegarde automatique

**`utils/api_client.py`** :
- Client HTTP pour l'API FastAPI
- UtilisÃ© par Streamlit pour appeler l'API
- Gestion des erreurs et timeouts

---

### Phase 5 : Dockerisation ğŸ³

#### 5.1 StratÃ©gie de Conteneurisation

**Architecture multi-services** :
```
Docker Compose
â”œâ”€ Service Streamlit (Port 8501)
â”‚  â””â”€ Interface utilisateur
â””â”€ Service API (Port 8000)
   â””â”€ Backend FastAPI
```

#### 5.2 Dockerfile

```dockerfile
FROM python:3.10-slim

WORKDIR /app

# Installation des dÃ©pendances
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copie du code
COPY . .

# CrÃ©ation des dossiers
RUN mkdir -p data models logs

# Exposition des ports
EXPOSE 8501 8000

# Script d'entrÃ©e
ENTRYPOINT ["/docker-entrypoint.sh"]
```

#### 5.3 Docker Compose

```yaml
version: '3.8'

services:
  streamlit:
    build: .
    ports:
      - "8501:8501"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - SERVICE_MODE=streamlit
    command: streamlit run app.py --server.address=0.0.0.0

  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./models:/app/models
    environment:
      - SERVICE_MODE=api
    command: uvicorn api.main:app --host 0.0.0.0 --port 8000
```

#### 5.4 Volumes Persistants

```
Machine HÃ´te          Conteneur Docker
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€         â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
./data/          â†’    /app/data/
./models/        â†’    /app/models/
./logs/          â†’    /app/logs/

Avantages :
â”œâ”€ DonnÃ©es persistent aprÃ¨s redÃ©marrage
â”œâ”€ ModÃ¨les persistent aprÃ¨s redÃ©marrage
â””â”€ Pas besoin de rebuild si donnÃ©es changent
```

---

## âœ¨ FonctionnalitÃ©s

### ğŸ” Analyse de DonnÃ©es
- Visualisations interactives (Plotly)
- Filtres dynamiques par code postal, type de bÃ¢timent, Ã©tiquette
- Statistiques descriptives
- Analyse gÃ©ographique

### ğŸ¤– Machine Learning
- **Classification** : PrÃ©diction de l'Ã©tiquette DPE (A-G) avec 98% de prÃ©cision
- **RÃ©gression** : Estimation du coÃ»t Ã©nergÃ©tique avec RÂ²=0.979
- PrÃ©dictions individuelles et par lot
- Intervalles de confiance

### ğŸ”„ Gestion des DonnÃ©es
- RafraÃ®chissement automatique depuis l'API ADEME
- Mode incrÃ©mental (nouveaux DPE uniquement)
- Mode complet (rechargement total)
- Gestion de 2 sources (existants + neufs)
- Fusion et harmonisation automatiques

### ğŸ¯ RÃ©entraÃ®nement
- Interface pour rÃ©entraÃ®ner les modÃ¨les
- Configuration des hyperparamÃ¨tres
- Visualisation des performances en temps rÃ©el
- Sauvegarde automatique

### ğŸ“¡ API REST
- 8 endpoints documentÃ©s (Swagger)
- PrÃ©dictions via HTTP POST
- MÃ©triques des modÃ¨les
- Gestion asynchrone

---

## ğŸ—ï¸ Architecture

### Stack Technique

```
Frontend  : Streamlit 1.28+
Backend   : FastAPI 0.104+
ML        : Scikit-learn 1.3+
Viz       : Plotly 5.17+
Data      : Pandas 2.1+, NumPy 1.25+
Deploy    : Docker, Docker Compose
```

### Flux de DonnÃ©es

```
API ADEME â†’ CSV â†’ Nettoyage â†’ ML â†’ ModÃ¨les .pkl â†’ API â†’ Interface
   â†“                                     â†‘
Notebooks                          RÃ©entraÃ®nement
```

---

## ğŸš€ Installation

### PrÃ©requis

- Python 3.10+
- pip
- Docker & Docker Compose (optionnel)
- 4 GB RAM minimum
- 2 GB d'espace disque

### Installation Locale

```bash
# 1. Cloner le repository
git clone https://github.com/votre-username/greentech-solutions.git
cd greentech-solutions

# 2. CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# 3. Installer les dÃ©pendances
pip install -r requirements.txt

# 4. CrÃ©er les dossiers nÃ©cessaires
mkdir -p data models logs
```

### Installation Docker

```bash
# 1. Cloner le repository
git clone https://github.com/votre-username/greentech-solutions.git
cd greentech-solutions

# 2. Construire les images
docker-compose build

# 3. Lancer les services
docker-compose up -d
```

---

## ğŸ’» Utilisation

### Lancement Local

```bash
# Terminal 1 : Lancer Streamlit
streamlit run app.py

# Terminal 2 : Lancer l'API
uvicorn api.main:app --reload
```

Puis ouvrir :
- **Streamlit** : http://localhost:8501
- **API Docs** : http://localhost:8000/docs

### Lancement Docker

```bash
# DÃ©marrer tous les services
docker-compose up -d

# Voir les logs
docker-compose logs -f

# ArrÃªter les services
docker-compose down
```

### Workflow Typique

```bash
# 1. PremiÃ¨re utilisation : EntraÃ®ner les modÃ¨les
â†’ Ouvrir http://localhost:8501
â†’ Aller sur "ğŸ¯ RÃ©entraÃ®ner modÃ¨les"
â†’ Cliquer "Lancer l'entraÃ®nement"
â†’ Attendre 1-2 minutes

# 2. Faire des prÃ©dictions
â†’ Aller sur "ğŸ”® PrÃ©diction"
â†’ Remplir le formulaire
â†’ Cliquer "Lancer la prÃ©diction"

# 3. RafraÃ®chir les donnÃ©es (optionnel)
â†’ Aller sur "ğŸ”„ RafraÃ®chir donnÃ©es"
â†’ Choisir mode incrÃ©mental ou complet
â†’ Lancer le rafraÃ®chissement

# 4. Utiliser l'API
â†’ Ouvrir http://localhost:8000/docs
â†’ Tester les endpoints interactivement
```

---

## ğŸ³ Dockerisation

### Commandes Utiles

```bash
# Construire
docker-compose build

# DÃ©marrer
docker-compose up -d

# ArrÃªter
docker-compose down

# Logs
docker-compose logs -f streamlit
docker-compose logs -f api

# RedÃ©marrer un service
docker-compose restart api

# Entrer dans un conteneur
docker-compose exec streamlit bash

# Reconstruire et redÃ©marrer
docker-compose up -d --build
```

### Push vers Docker Hub

```bash
# Tag
docker tag greentech-app:latest username/greentech-app:latest

# Push
docker push username/greentech-app:latest

# Pull (sur un autre serveur)
docker pull username/greentech-app:latest
docker-compose up -d
```

---

## ğŸ“ Structure du Projet

```
greentech-solutions/
â”‚
â”œâ”€â”€ ğŸ““ notebooks/                    # Notebooks Jupyter
â”‚   â”œâ”€â”€ 01_collecte_donnees.ipynb   # RÃ©cupÃ©ration API ADEME + Enedis
â”‚   â”œâ”€â”€ 02_exploration_nettoyage.ipynb  # EDA et preprocessing
â”‚   â””â”€â”€ 03_modelisation.ipynb       # Classification et rÃ©gression
â”‚
â”œâ”€â”€ ğŸ¨ pages/                        # Pages Streamlit
â”‚   â”œâ”€â”€ welcome.py                  # Page d'accueil
â”‚   â”œâ”€â”€ home.py                     # Tableau de bord
â”‚   â”œâ”€â”€ analysis.py                 # Analyses
â”‚   â”œâ”€â”€ enedis.py                   # DonnÃ©es Enedis
â”‚   â”œâ”€â”€ prediction.py               # PrÃ©dictions
â”‚   â”œâ”€â”€ compare.py                  # Comparaisons
â”‚   â”œâ”€â”€ refresh_data.py             # RafraÃ®chissement donnÃ©es
â”‚   â”œâ”€â”€ retrain_models.py           # RÃ©entraÃ®nement modÃ¨les
â”‚   â”œâ”€â”€ api_interface.py            # Interface API
â”‚   â””â”€â”€ about.py                    # Ã€ propos
â”‚
â”œâ”€â”€ ğŸ”§ utils/                        # Modules utilitaires
â”‚   â”œâ”€â”€ data_loader.py              # Chargement donnÃ©es
â”‚   â”œâ”€â”€ model_utils.py              # Utilitaires modÃ¨les
â”‚   â”œâ”€â”€ data_refresher_complete.py  # RafraÃ®chissement API
â”‚   â”œâ”€â”€ model_trainer.py            # EntraÃ®nement modÃ¨les
â”‚   â””â”€â”€ api_client.py               # Client API
â”‚
â”œâ”€â”€ ğŸ”Œ api/                          # API FastAPI
â”‚   â””â”€â”€ main.py                     # Endpoints REST
â”‚
â”œâ”€â”€ ğŸ¤– models/                       # ModÃ¨les ML
â”‚   â”œâ”€â”€ classification_model.pkl    # ModÃ¨le classification
â”‚   â”œâ”€â”€ regression_model.pkl        # ModÃ¨le rÃ©gression
â”‚   â””â”€â”€ metrics.json                # MÃ©triques
â”‚
â”œâ”€â”€ ğŸ“Š data/                         # DonnÃ©es
â”‚   â”œâ”€â”€ donnees_ademe_finales_*.csv # Dataset principal
â”‚   â”œâ”€â”€ adresses-69.csv             # Codes postaux
â”‚   â””â”€â”€ metadata.json               # MÃ©tadonnÃ©es
â”‚
â”œâ”€â”€ ğŸ³ Docker/                       # Configuration Docker
â”‚   â”œâ”€â”€ Dockerfile                  # Image Docker
â”‚   â”œâ”€â”€ docker-compose.yml          # Orchestration
â”‚   â””â”€â”€ docker-entrypoint.sh        # Script dÃ©marrage
â”‚
â”œâ”€â”€ ğŸ“± app.py                        # Application principale
â”œâ”€â”€ ğŸ“‹ requirements.txt              # DÃ©pendances Python
â”œâ”€â”€ ğŸ§ª test_api.py                  # Tests API
â””â”€â”€ ğŸ“– README.md                    # Ce fichier
```

---

## ğŸ› ï¸ Technologies UtilisÃ©es

### Backend & API
![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)
![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green?logo=fastapi)
![Pandas](https://img.shields.io/badge/Pandas-2.1-orange?logo=pandas)
![NumP
